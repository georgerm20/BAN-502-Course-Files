---
output:
  word_document: default
  html_document: default
---
# George Germano
## Module 4 - Assignment 2

Library
```{r}
options(tidyverse.quiet = TRUE)
library(tidyverse)
library(rpart)
library(RColorBrewer)
library(rattle)
library(caret)
```

Before beginning the assignment tasks, you should read-in the data for the assignment into a data frame called parole.
```{r}
parole <- read_csv("parole.csv")
```


Carefully convert the male, race, state, crime, multiple.offenses, and violator variables to
factors. Recode (rename) the factor levels of each of these variables according to the description of the variables provided in the ParoleData.txt file (located with the assignment on Canvas).
```{r}
parole = parole %>% 
  mutate(male = as.factor(male)) %>% mutate(male = fct_recode(male, "female" = "0", "male" = "1" )) %>%
  mutate(race = as.factor(race)) %>% mutate(race = fct_recode(race, "white" = "1", "other_race" = "2" )) %>%
  mutate(state = as.factor(state)) %>% mutate(state = fct_recode(state, "other_state" = "1", "Kentuck" = "2", "Louisiana" = "3", "Virginia" = "4" )) %>%
  mutate(crime = as.factor(crime)) %>% mutate(crime = fct_recode(crime, "other_crime" = "1", "larceny" = "2", "drug-related" = "3", "driving-related" = "4" )) %>%
  mutate(multiple.offenses = as.factor(multiple.offenses)) %>% mutate(multiple.offenses = fct_recode(multiple.offenses, "no" = "0", "yes" = "1" )) %>%
  mutate(violator = as.factor(violator)) %>% mutate(violator = fct_recode(violator, "no" = "0", "yes" = "1" ))
```


Task 1: Split the data into training and testing sets. Your training set should have 70% of the data. Use a random number (set.seed) of 12345.
```{r}
set.seed(12345) 
train.rows = createDataPartition(y = parole$violator, p=0.7, list = FALSE)
train = slice(parole, train.rows)
test = slice(parole, -train.rows)
```

Task 2: Create a classification tree using all of the predictor variables to predict “violator” in the training set. Plot the tree.
```{r}
tree1 = rpart(violator  ~., train, method="class")
fancyRpartPlot(tree1)
```

Task 3: For the tree created in Task 2, how would you classify a 40 year-old parolee from Louisiana who served a 5 year prison sentence? Describe how you “walk through” the classification tree to arrive at your answer.

We start at the top of the tree and move right since the woman is from Louisianna. If the woman's race is not white, we go to the bottom left under "no" meaning no parole violation. If the woman is white, we move down to the right. From here we know she served five years, so we move to the far bottom right. As a result, we'd say yes to violating parole.

Task 4: Use the printcp function to evaluate tree performance as a function of the complexity parameter (cp). What cp value should be selected? Note that the printcp table tends to be a more reliable tool than the plot of cp.
```{r}
printcp(tree1)
plotcp(tree1)
```

The complexity parameter (cp) with the lowest cross-validation error is 0.030303, meaning that it should be selected. The lower cp's result in cross-validation error to continually rise above 1.0.

Task 5: Task 5: Prune the tree from Task 2 back to the cp value that you selected in Task 4. Do not attempt to plot the tree. You will find that the resulting tree is known as a “root”. A tree that takes the form of a root is essentially a naive model that assumes that the prediction for all observations is the majority class. Which class (category) in the training set is the majority class (i.e., has the most observations)?
```{r}
tree2 = prune(tree1,cp= tree1$cptable[which.min(tree1$cptable[,"xerror"]),"CP"])
summary(train)
```
The majority class in the train data is "no" for no parole violations.

Task 6: Use the unpruned tree from Task 2 to develop predictions for the training data. Use caret’s confusionMatrix function to calculate the accuracy, specificity, and sensitivty of this tree on the training data. Note that we would not, in practice, use an unpruned tree as such a tree is very likely to overfit on new data.
```{r}
treepred = predict(tree1, train, type = "class")

confusionMatrix(treepred,train$violator,positive="yes")
```

Task 7: Use the unpruned tree from Task 2 to develop predictions for the testing data. Use caret’s confusionMatrix function to calculate the accuracy, specificity, and sensitivty of this tree on the testing data. Comment on the quality of the model.
```{r}
treepred_test = predict(tree1, newdata=test, type = "class")

confusionMatrix(treepred_test,test$violator,positive="yes")
```

The accuracy is higher than the naive model in the train and test datasets. Additionally, both the training and test models have similar accuracies and "no infomration rate" values, meaning the data is likely not overfit.

Task 8: Read in the “Blood.csv” dataset. Convert the DonatedMarch variable to a factor and recode the variable so 0 = “No” and 1 = “Yes”.
```{r}
Blood <- read_csv("Blood.csv")

Blood = Blood %>%
  mutate(DonatedMarch = as.factor(DonatedMarch)) %>% mutate(DonatedMarch = fct_recode(DonatedMarch, "No" = "0", "Yes" = "1" ))
```

Task 9: Split the dataset into training (70%) and testing (30%) sets. You may wish to name your
training and testing sets “train2” and “test2” so as to not confuse them with the parole datsets
Use set.seed of 1234. Then develop a classification tree on the training set to predict “DonatedMarch”. Evaluate the complexity parameter (cp) selection for this model.
```{r}
set.seed(1234) 
train.rows2 = createDataPartition(y = Blood$DonatedMarch, p=0.7, list = FALSE)
train2 = slice(Blood, train.rows2)
test2 = slice(Blood, -train.rows2)
```

```{r}
tree3 = rpart(DonatedMarch ~., train2, method="class")
fancyRpartPlot(tree3)
printcp(tree3)
plotcp(tree3)
```

The complexity parameter that should be used with this data is .01 as the cross-validation error is at it's lowest of the two options. 

Task 10: Prune the tree back to the optimal cp value, make predictions, and use the confusionMatrix function on the both training and testing sets. Comment on the quality of the predictions.
```{r}
tree4 = prune(tree3,cp= tree1$cptable[which.min(tree1$cptable[,"xerror"]),"CP"]) #Note, we were already using the optimal cp value. 
fancyRpartPlot(tree3)
```

```{r Train 2}
treepred2 = predict(tree4, train2, type = "class")

confusionMatrix(treepred2,train2$DonatedMarch,positive="Yes")
```

```{r Test 2}
treepred_test2 = predict(tree4, newdata=test2, type = "class")

confusionMatrix(treepred_test2,test2$DonatedMarch,positive="Yes")
```

The train and test datasets both have identical no information rates and slightly differing accuracies, meaning there's a good chance there is no data overfit. In both instances, the accuracy is higher than the naive model, making it appear to be a good fit.